{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM1tYPlb4DkveQU4qgbrMF8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"twZDkoA80o8e"},"source":["import warnings\n","import pandas as pd\n","import os\n","import librosa\n","import numpy as np\n","import math\n","import multiprocessing\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","import scipy\n","import pickle\n","import IPython.display as ipd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xo9ACHKT0prf"},"source":["np.random.seed(999)\n","tf.random.set_seed(999)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cDQW8Brk1OkH","executionInfo":{"status":"ok","timestamp":1606241250510,"user_tz":360,"elapsed":16174,"user":{"displayName":"Daniel Bates","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYC5J7YO7520gpn4TOpt0AqNjXaoFLJcT5rDeM8Q=s64","userId":"11979675232697855228"}},"outputId":"1035231b-98f7-4d13-9b1a-1aeb5720f59e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j8-LdUYy1OH1"},"source":["# Set your drive's path for the files used.\n","maleVoicePath = '/content/drive/My Drive/ML Project/.csv'\n","whiteNoisePath = '/content/drive/My Drive/ML Project/.csv'\n","DrivePath = '/content/drive/My Drive/ML Project'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vt8kS0M0tTF"},"source":["Utilities Functions"]},{"cell_type":"code","metadata":{"id":"HyLR9WXW0sU9"},"source":["def inverse_stft_transform(stft_features, window_length, overlap):\n","    return librosa.istft(stft_features, win_length=window_length, hop_length=overlap)\n","\n","def revert_features_to_audio(features, phase, window_length, overlap, cleanMean=None, cleanStd=None):\n","    # scale the outpus back to the original range\n","    if cleanMean and cleanStd:\n","        features = cleanStd * features + cleanMean\n","\n","    phase = np.transpose(phase, (1, 0))\n","    features = np.squeeze(features)\n","    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n","\n","    features = np.transpose(features, (1, 0))\n","    return inverse_stft_transform(features, window_length=window_length, overlap=overlap)\n","\n","def play(audio, sample_rate):\n","    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file\n","\n","def add_noise_to_clean_audio(clean_audio, noise_signal):\n","    if len(clean_audio) >= len(noise_signal):\n","        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n","        while len(clean_audio) >= len(noise_signal):\n","            noise_signal = np.append(noise_signal, noise_signal)\n","\n","    ## Extract a noise segment from a random location in the noise file\n","    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n","\n","    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n","\n","    speech_power = np.sum(clean_audio ** 2)\n","    noise_power = np.sum(noiseSegment ** 2)\n","    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n","    return noisyAudio\n","\n","def read_audio(filepath, sample_rate, normalize=True):\n","    audio, sr = librosa.load(filepath, sr=sample_rate)\n","    if normalize is True:\n","        div_fac = 1 / np.max(np.abs(audio)) / 3.0\n","        audio = audio * div_fac\n","        # audio = librosa.util.normalize(audio)\n","    return audio, sr\n","\n","def prepare_input_features(stft_features, numSegments, numFeatures):\n","    noisySTFT = np.concatenate([stft_features[:, 0:numSegments - 1], stft_features], axis=1)\n","    stftSegments = np.zeros((numFeatures, numSegments, noisySTFT.shape[1] - numSegments + 1))\n","\n","    for index in range(noisySTFT.shape[1] - numSegments + 1):\n","        stftSegments[:, :, index] = noisySTFT[:, index:index + numSegments]\n","    return stftSegments\n","'''\n","def get_input_features(predictorsList):\n","    predictors = []\n","    for noisy_stft_mag_features in predictorsList:\n","        # For CNN, the input feature consisted of 8 consecutive noisy\n","        # STFT magnitude vectors of size: 129 Ã— 8,\n","        # TODO: duration: 100ms\n","        inputFeatures = prepare_input_features(noisy_stft_mag_features)\n","        # print(\"inputFeatures.shape\", inputFeatures.shape)\n","        predictors.append(inputFeatures)\n","    return predictors\n","'''\n","def _bytes_feature(value):\n","    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n","    if isinstance(value, type(tf.constant(0))):\n","        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","def _float_feature(value):\n","    \"\"\"Returns a float_list from a float / double.\"\"\"\n","    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n","\n","def _int64_feature(value):\n","    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n","    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n","\n","def get_tf_feature(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n","    noise_stft_mag_features = noise_stft_mag_features.astype(np.float32).tostring()\n","    clean_stft_magnitude = clean_stft_magnitude.astype(np.float32).tostring()\n","    noise_stft_phase = noise_stft_phase.astype(np.float32).tostring()\n","\n","    example = tf.train.Example(features=tf.train.Features(feature={\n","        'noise_stft_phase': _bytes_feature(noise_stft_phase),\n","        'noise_stft_mag_features': _bytes_feature(noise_stft_mag_features),\n","        'clean_stft_magnitude': _bytes_feature(clean_stft_magnitude)}))\n","    return example\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOxIstrR0u13"},"source":["Feature Extractor"]},{"cell_type":"code","metadata":{"id":"TS20ZPpm0v8C"},"source":["class FeatureExtractor:\n","    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n","        self.audio = audio\n","        self.ffT_length = windowLength\n","        self.window_length = windowLength\n","        self.overlap = overlap\n","        self.sample_rate = sample_rate\n","        self.window = scipy.signal.hamming(self.window_length, sym=False)\n","\n","    def get_stft_spectrogram(self):\n","        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n","                            window=self.window, center=True)\n","\n","    def get_audio_from_stft_spectrogram(self, stft_features):\n","        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n","                             window=self.window, center=True)\n","\n","    def get_mel_spectrogram(self):\n","        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n","                                              n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n","\n","    def get_audio_from_mel_spectrogram(self, M):\n","        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length,\n","                                                    hop_length=self.overlap,\n","                                                    win_length=self.window_length, window=self.window,\n","                                                    center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jiYK7240zTy"},"source":["Base Dataset Class"]},{"cell_type":"code","metadata":{"id":"Zd1I7fj40y7d"},"source":["class Dataset:\n","    def __init__(self, clean_filenames, noise_filenames, **config):\n","        self.clean_filenames = clean_filenames\n","        self.noise_filenames = noise_filenames\n","        self.sample_rate = config['fs']\n","        self.overlap = config['overlap']\n","        self.window_length = config['windowLength']\n","        self.audio_max_duration = config['audio_max_duration']\n","\n","    def _sample_noise_filename(self):\n","        return np.random.choice(self.noise_filenames)\n","\n","    def _remove_silent_frames(self, audio):\n","        trimed_audio = []\n","        indices = librosa.effects.split(audio, hop_length=self.overlap, top_db=20)\n","\n","        for index in indices:\n","            trimed_audio.extend(audio[index[0]: index[1]])\n","        return np.array(trimed_audio)\n","\n","    def _phase_aware_scaling(self, clean_spectral_magnitude, clean_phase, noise_phase):\n","        assert clean_phase.shape == noise_phase.shape, \"Shapes must match.\"\n","        return clean_spectral_magnitude * np.cos(clean_phase - noise_phase)\n","\n","    def get_noisy_audio(self, *, filename):\n","        return read_audio(filename, self.sample_rate)\n","\n","    def _audio_random_crop(self, audio, duration):\n","        audio_duration_secs = librosa.core.get_duration(audio, self.sample_rate)\n","\n","        ## duration: length of the cropped audio in seconds\n","        if duration >= audio_duration_secs:\n","            # print(\"Passed duration greater than audio duration of: \", audio_duration_secs)\n","            return audio\n","\n","        audio_duration_ms = math.floor(audio_duration_secs * self.sample_rate)\n","        duration_ms = math.floor(duration * self.sample_rate)\n","        idx = np.random.randint(0, audio_duration_ms - duration_ms)\n","        return audio[idx: idx + duration_ms]\n","\n","    def _add_noise_to_clean_audio(self, clean_audio, noise_signal):\n","        if len(clean_audio) >= len(noise_signal):\n","            # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n","            while len(clean_audio) >= len(noise_signal):\n","                noise_signal = np.append(noise_signal, noise_signal)\n","\n","        ## Extract a noise segment from a random location in the noise file\n","        ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n","\n","        noiseSegment = noise_signal[ind: ind + clean_audio.size]\n","\n","        speech_power = np.sum(clean_audio ** 2)\n","        noise_power = np.sum(noiseSegment ** 2)\n","        noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n","        return noisyAudio\n","\n","    def parallel_audio_processing(self, clean_filename):\n","        clean_audio, _ = read_audio(clean_filename, self.sample_rate)\n","\n","        # remove silent frame from clean audio\n","        clean_audio = self._remove_silent_frames(clean_audio)\n","\n","        noise_filename = self._sample_noise_filename()\n","\n","        # read the noise filename\n","        noise_audio, sr = read_audio(noise_filename, self.sample_rate)\n","\n","        # remove silent frame from noise audio\n","        noise_audio = self._remove_silent_frames(noise_audio)\n","\n","        # sample random fixed-sized snippets of audio\n","        clean_audio = self._audio_random_crop(clean_audio, duration=self.audio_max_duration)\n","\n","        # add noise to input image\n","        noiseInput = self._add_noise_to_clean_audio(clean_audio, noise_audio)\n","\n","        # extract stft features from noisy audio\n","        noisy_input_fe = FeatureExtractor(noiseInput, windowLength=self.window_length, overlap=self.overlap,\n","                                          sample_rate=self.sample_rate)\n","        noise_spectrogram = noisy_input_fe.get_stft_spectrogram()\n","\n","        # Or get the phase angle (in radians)\n","        # noisy_stft_magnitude, noisy_stft_phase = librosa.magphase(noisy_stft_features)\n","        noise_phase = np.angle(noise_spectrogram)\n","\n","        # get the magnitude of the spectral\n","        noise_magnitude = np.abs(noise_spectrogram)\n","\n","        # extract stft features from clean audio\n","        clean_audio_fe = FeatureExtractor(clean_audio, windowLength=self.window_length, overlap=self.overlap,\n","                                          sample_rate=self.sample_rate)\n","        clean_spectrogram = clean_audio_fe.get_stft_spectrogram()\n","        # clean_spectrogram = cleanAudioFE.get_mel_spectrogram()\n","\n","        # get the clean phase\n","        clean_phase = np.angle(clean_spectrogram)\n","\n","        # get the clean spectral magnitude\n","        clean_magnitude = np.abs(clean_spectrogram)\n","        # clean_magnitude = 2 * clean_magnitude / np.sum(scipy.signal.hamming(self.window_length, sym=False))\n","\n","        clean_magnitude = self._phase_aware_scaling(clean_magnitude, clean_phase, noise_phase)\n","\n","        scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n","        noise_magnitude = scaler.fit_transform(noise_magnitude)\n","        clean_magnitude = scaler.transform(clean_magnitude)\n","\n","        return noise_magnitude, clean_magnitude, noise_phase\n","\n","    def create_tf_record(self, *, prefix, subset_size, parallel=True):\n","        counter = 0\n","        p = multiprocessing.Pool(multiprocessing.cpu_count())\n","\n","        for i in range(0, len(self.clean_filenames), subset_size):\n","\n","            #tfrecord_filename = './records/' + prefix + '_' + str(counter) + '.tfrecords'\n","            tfrecord_filename = '/content/drive/My Drive/ML Project/records/' + prefix + '_' + str(counter) + '.tfrecords'\n","            if os.path.isfile(tfrecord_filename):\n","                print(f\"Skipping {tfrecord_filename}\")\n","                counter += 1\n","                continue\n","\n","            writer = tf.io.TFRecordWriter(tfrecord_filename)\n","            clean_filenames_sublist = self.clean_filenames[i:i + subset_size]\n","\n","            print(f\"Processing files from: {i} to {i + subset_size}\")\n","            if parallel:\n","                out = p.map(self.parallel_audio_processing, clean_filenames_sublist)\n","            else:\n","                out = [self.parallel_audio_processing(filename) for filename in clean_filenames_sublist]\n","            for o in out:\n","                noise_stft_magnitude = o[0]\n","                clean_stft_magnitude = o[1]\n","                noise_stft_phase = o[2]\n","\n","                noise_stft_mag_features = prepare_input_features(noise_stft_magnitude, numSegments=8, numFeatures=129)\n","\n","                noise_stft_mag_features = np.transpose(noise_stft_mag_features, (2, 0, 1))\n","                clean_stft_magnitude = np.transpose(clean_stft_magnitude, (1, 0))\n","                noise_stft_phase = np.transpose(noise_stft_phase, (1, 0))\n","\n","                noise_stft_mag_features = np.expand_dims(noise_stft_mag_features, axis=3)\n","                clean_stft_magnitude = np.expand_dims(clean_stft_magnitude, axis=2)\n","\n","                for x_, y_, p_ in zip(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n","                    y_ = np.expand_dims(y_, 2)\n","                    example = get_tf_feature(x_, y_, p_)\n","                    writer.write(example.SerializeToString())\n","\n","            counter += 1\n","            writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7N1NzjXu04LQ"},"source":["Common Voice Class"]},{"cell_type":"code","metadata":{"id":"tlKYKAKY04wk"},"source":["class MozillaCommonVoiceDataset:\n","\n","    def __init__(self, basepath, *, val_dataset_size):\n","        self.basepath = basepath\n","        self.val_dataset_size = val_dataset_size\n","\n","    def _get_common_voice_filenames(self, dataframe_name='male.csv'):\n","        mozilla_metadata = pd.read_csv(os.path.join(self.basepath, dataframe_name))\n","        clean_files = mozilla_metadata['File'].values\n","        np.random.shuffle(clean_files)\n","        print(\"Total number of training examples:\", len(clean_files))\n","        return clean_files\n","\n","    def get_train_val_filenames(self):\n","        clean_files = self._get_common_voice_filenames(dataframe_name='male.csv')\n","\n","        # resolve full path\n","        clean_files = [os.path.join(DrivePath, 'male', filename) for filename in clean_files]\n","        #'/content/drive/My Drive/ML Project/.csv/clips/test/common_voice_en_22238323.mp3\n","        clean_files = clean_files[:-self.val_dataset_size]\n","        clean_val_files = clean_files[-self.val_dataset_size:]\n","        print(\"# of Training clean files:\", len(clean_files))\n","        print(\"# of  Validation clean files:\", len(clean_val_files))\n","        return clean_files, clean_val_files\n","\n","\n","    def get_test_filenames(self):\n","        clean_files = self._get_common_voice_filenames(dataframe_name='male.csv')\n","\n","        # resolve full path\n","        clean_files = [os.path.join(DrivePath, 'male', filename) for filename in clean_files]\n","\n","        print(\"# of Testing clean files:\", len(clean_files))\n","        return clean_files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmB_2BUm07XW"},"source":["WhiteNoise Class"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tU76tEZE076n"},"source":["class WhiteNoise:\n","    def __init__(self, basepath, *, val_dataset_size, class_ids=None):\n","        self.basepath = basepath\n","        self.val_dataset_size = val_dataset_size\n","        self.class_ids = class_ids\n","\n","    def _get_whitenoise_filenames(self):\n","        whitenoise_metadata = pd.read_csv(os.path.join(self.basepath, 'WhiteNoise.csv'))\n","\n","        # shuffle the dataframe\n","        whitenoise_metadata.reindex(np.random.permutation(whitenoise_metadata.index))\n","\n","        return whitenoise_metadata\n","\n","    def _get_filenames_by_class_id(self, metadata):\n","\n","        if self.class_ids is None:\n","            self.class_ids = np.unique(metadata['classID'].values)\n","            print(\"Number of classes:\", self.class_ids)\n","\n","        all_files = []\n","        file_counter = 0\n","        for c in self.class_ids:\n","            per_class_files = metadata[metadata['classID'] == c][['slice_file_name', 'fold']].values\n","            per_class_files = [os.path.join(self.basepath, 'audio', 'fold' + str(file[1]), file[0]) for file in\n","                               per_class_files]\n","            print(\"Class c:\", str(c), 'has:', len(per_class_files), 'files')\n","            file_counter += len(per_class_files)\n","            all_files.extend(per_class_files)\n","\n","        assert len(all_files) == file_counter\n","        return all_files\n","\n","    def get_train_val_filenames(self):\n","        whitenoise_metadata = self._get_whitenoise_filenames()\n","        '''\n","        # folds from 0 to 9 are used for training\n","        whitenoise_train = whitenoise_metadata[whitenoise_metadata.fold != 10]\n","\n","        whitenoise_train_filenames = self._get_filenames_by_class_id(whitenoise_train)\n","        np.random.shuffle(whitenoise_train_filenames)\n","        '''\n","        # separate noise files for train/validation\n","        whitenoise_train_filenames = whitenoise_metadata['File'].values\n","        np.random.shuffle(whitenoise_train_filenames)\n","\n","        whitenoise_val = whitenoise_train_filenames[-self.val_dataset_size:]\n","        whitenoise_train = whitenoise_train_filenames[:-self.val_dataset_size]\n","\n","        return whitenoise_train, whitenoise_val\n","\n","    def get_test_filenames(self):\n","        whitenoise_metadata = self._get_whitenoise_filenames()\n","        whitenoise_test_filenames = whitenoise_metadata['File'].values\n","        '''\n","        # fold 10 is used for testing only\n","        whitenoise_train = whitenoise_metadata[whitenoise_metadata.fold == 10]\n","        '''\n","        #whitenoise_test_filenames = self._get_filenames_by_class_id(whitenoise_train)\n","        np.random.shuffle(whitenoise_test_filenames)\n","\n","        print(\"# of Noise testing files:\", len(whitenoise_test_filenames))\n","        return whitenoise_test_filenames"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vHlIc6a0_Ya"},"source":["Final Script"]},{"cell_type":"code","metadata":{"id":"Tf-GmrL60lL9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606088662845,"user_tz":360,"elapsed":2248784,"user":{"displayName":"Daniel Bates","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYC5J7YO7520gpn4TOpt0AqNjXaoFLJcT5rDeM8Q=s64","userId":"11979675232697855228"}},"outputId":"75a03f9c-5f10-4122-9393-20a2a9c7233d"},"source":["warnings.filterwarnings(action='ignore')\n","\n","mozilla_basepath = maleVoicePath\n","whitenoise_basepath = whiteNoisePath\n","\n","mcv = MozillaCommonVoiceDataset(mozilla_basepath, val_dataset_size=500)\n","clean_train_filenames, clean_val_filenames = mcv.get_train_val_filenames()\n","\n","wn = WhiteNoise(whitenoise_basepath, val_dataset_size=5)\n","noise_train_filenames, noise_val_filenames = wn.get_train_val_filenames()\n","\n","windowLength = 256\n","config = {'windowLength': windowLength,\n","          'overlap': round(0.25 * windowLength),\n","          'fs': 16000,\n","          'audio_max_duration': 0.8}\n","print(noise_val_filenames)\n","val_dataset = Dataset(clean_val_filenames, noise_val_filenames, **config)\n","val_dataset.create_tf_record(prefix='val', subset_size=30)\n","\n","train_dataset = Dataset(clean_train_filenames, noise_train_filenames, **config)\n","train_dataset.create_tf_record(prefix='train', subset_size=50)\n","\n","## Create Test Set\n","clean_test_filenames = mcv.get_test_filenames()\n","\n","noise_test_filenames = wn.get_test_filenames()\n","\n","test_dataset = Dataset(clean_test_filenames, noise_test_filenames, **config)\n","test_dataset.create_tf_record(prefix='test', subset_size=10, parallel=False)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total number of training examples: 1000\n","# of Training clean files: 500\n","# of  Validation clean files: 500\n","['/content/drive/My Drive/ML Project/whitenoise/wn7.mp3'\n"," '/content/drive/My Drive/ML Project/whitenoise/wn2.mp3'\n"," '/content/drive/My Drive/ML Project/whitenoise/wn10.mp3'\n"," '/content/drive/My Drive/ML Project/whitenoise/wn3.mp3'\n"," '/content/drive/My Drive/ML Project/whitenoise/wn8.mp3']\n","Processing files from: 0 to 30\n","Processing files from: 30 to 60\n","Processing files from: 60 to 90\n","Processing files from: 90 to 120\n","Processing files from: 120 to 150\n","Processing files from: 150 to 180\n","Processing files from: 180 to 210\n","Processing files from: 210 to 240\n","Processing files from: 240 to 270\n","Processing files from: 270 to 300\n","Processing files from: 300 to 330\n","Processing files from: 330 to 360\n","Processing files from: 360 to 390\n","Processing files from: 390 to 420\n","Processing files from: 420 to 450\n","Processing files from: 450 to 480\n","Processing files from: 480 to 510\n","Processing files from: 0 to 50\n","Processing files from: 50 to 100\n","Processing files from: 100 to 150\n","Processing files from: 150 to 200\n","Processing files from: 200 to 250\n","Processing files from: 250 to 300\n","Processing files from: 300 to 350\n","Processing files from: 350 to 400\n","Processing files from: 400 to 450\n","Processing files from: 450 to 500\n","Total number of training examples: 1000\n","# of Testing clean files: 1000\n","# of Noise testing files: 10\n","Processing files from: 0 to 10\n","Processing files from: 10 to 20\n","Processing files from: 20 to 30\n","Processing files from: 30 to 40\n","Processing files from: 40 to 50\n","Processing files from: 50 to 60\n","Processing files from: 60 to 70\n","Processing files from: 70 to 80\n","Processing files from: 80 to 90\n","Processing files from: 90 to 100\n","Processing files from: 100 to 110\n","Processing files from: 110 to 120\n","Processing files from: 120 to 130\n","Processing files from: 130 to 140\n","Processing files from: 140 to 150\n","Processing files from: 150 to 160\n","Processing files from: 160 to 170\n","Processing files from: 170 to 180\n","Processing files from: 180 to 190\n","Processing files from: 190 to 200\n","Processing files from: 200 to 210\n","Processing files from: 210 to 220\n","Processing files from: 220 to 230\n","Processing files from: 230 to 240\n","Processing files from: 240 to 250\n","Processing files from: 250 to 260\n","Processing files from: 260 to 270\n","Processing files from: 270 to 280\n","Processing files from: 280 to 290\n","Processing files from: 290 to 300\n","Processing files from: 300 to 310\n","Processing files from: 310 to 320\n","Processing files from: 320 to 330\n","Processing files from: 330 to 340\n","Processing files from: 340 to 350\n","Processing files from: 350 to 360\n","Processing files from: 360 to 370\n","Processing files from: 370 to 380\n","Processing files from: 380 to 390\n","Processing files from: 390 to 400\n","Processing files from: 400 to 410\n","Processing files from: 410 to 420\n","Processing files from: 420 to 430\n","Processing files from: 430 to 440\n","Processing files from: 440 to 450\n","Processing files from: 450 to 460\n","Processing files from: 460 to 470\n","Processing files from: 470 to 480\n","Processing files from: 480 to 490\n","Processing files from: 490 to 500\n","Processing files from: 500 to 510\n","Processing files from: 510 to 520\n","Processing files from: 520 to 530\n","Processing files from: 530 to 540\n","Processing files from: 540 to 550\n","Processing files from: 550 to 560\n","Processing files from: 560 to 570\n","Processing files from: 570 to 580\n","Processing files from: 580 to 590\n","Processing files from: 590 to 600\n","Processing files from: 600 to 610\n","Processing files from: 610 to 620\n","Processing files from: 620 to 630\n","Processing files from: 630 to 640\n","Processing files from: 640 to 650\n","Processing files from: 650 to 660\n","Processing files from: 660 to 670\n","Processing files from: 670 to 680\n","Processing files from: 680 to 690\n","Processing files from: 690 to 700\n","Processing files from: 700 to 710\n","Processing files from: 710 to 720\n","Processing files from: 720 to 730\n","Processing files from: 730 to 740\n","Processing files from: 740 to 750\n","Processing files from: 750 to 760\n","Processing files from: 760 to 770\n","Processing files from: 770 to 780\n","Processing files from: 780 to 790\n","Processing files from: 790 to 800\n","Processing files from: 800 to 810\n","Processing files from: 810 to 820\n","Processing files from: 820 to 830\n","Processing files from: 830 to 840\n","Processing files from: 840 to 850\n","Processing files from: 850 to 860\n","Processing files from: 860 to 870\n","Processing files from: 870 to 880\n","Processing files from: 880 to 890\n","Processing files from: 890 to 900\n","Processing files from: 900 to 910\n","Processing files from: 910 to 920\n","Processing files from: 920 to 930\n","Processing files from: 930 to 940\n","Processing files from: 940 to 950\n","Processing files from: 950 to 960\n","Processing files from: 960 to 970\n","Processing files from: 970 to 980\n","Processing files from: 980 to 990\n","Processing files from: 990 to 1000\n"],"name":"stdout"}]}]}